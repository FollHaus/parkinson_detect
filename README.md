# Классификация болезни Паркинсона с использованием XGBoost и стекинга

Этот проект демонстрирует, как классифицировать статус болезни Паркинсона (здоровый человек или больной Паркинсоном) с использованием моделей машинного обучения, таких как XGBoost и стекинговые классификаторы. В качестве набора данных используются различные характеристики, извлеченные из голосовых записей пациентов, а целевая переменная — статус болезни.

## Необходимые библиотеки

Для запуска проекта необходимо установить следующие библиотеки Python:

- **numpy**: Для работы с числовыми массивами.
- **pandas**: Для манипуляций с данными.
- **seaborn**: Для визуализации данных (необязательно).
- **xgboost**: Для модели классификации XGBoost.
- **scikit-learn**: Для различных алгоритмов машинного обучения и утилит (например, KNeighborsClassifier, RandomForestClassifier и StackClassifier).

Эти библиотеки можно установить с помощью pip:

```bash
pip install numpy pandas seaborn xgboost scikit-learn
```

## Набор данных

Для работы используется **набор данных о болезни Паркинсона**, который обычно доступен в репозитории машинного обучения UCI. Набор данных содержит 22 признака, представляющие различные характеристики голосовых данных, и целевую переменную `status`, где `1` означает наличие болезни Паркинсона, а `0` — отсутствие.

## Описание кода

### 1. **Загрузка данных**

```python
df = pd.read_csv('data/parkinsons.csv')
```

Данные загружаются с помощью pandas из CSV файла. Данные содержат признаки (характеристики голоса) и столбец целевой переменной `status`, который указывает, есть ли у человека болезнь Паркинсона (1) или нет (0).

### 2. **Предобработка данных**

```python
np_features = df.loc[:, df.columns != 'status'].values[:, 1:]
np_target = df.loc[:, 'status'].values

scaler = MinMaxScaler((-1, 1))
X = scaler.fit_transform(np_features)
y = np_target
```

- Признаки (`np_features`) извлекаются, исключая столбец `status`.
- Целевая переменная (`np_target`) извлекается из столбца `status`.
- Признаки масштабируются в диапазон от -1 до 1 с помощью `MinMaxScaler`, чтобы улучшить работу моделей машинного обучения.

### 3. **Разделение данных на обучающую и тестовую выборки**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
```

Данные делятся на обучающую (80%) и тестовую (20%) выборки. Аргумент `stratify=y` гарантирует, что распределение классов сохраняется как в обучающей, так и в тестовой выборках.

### 4. **Модель XGBoost**

```python
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    n_jobs=-1,
    random_state=42,
    tree_method="hist",
    colsample_bytree=0.7,
    gamma=0,
    learning_rate=0.2,
    max_depth=3,
    n_estimators=500,
    scale_pos_weight=1,
    subsample=0.9
)
xgb_model.fit(X_train, y_train)
```

Создается и обучается модель XGBoost с определенными гиперпараметрами. Некоторые важные гиперпараметры:
- `n_estimators`: Количество раундов бустинга (деревьев).
- `learning_rate`: Темп обучения (шаг обновления весов).
- `max_depth`: Максимальная глубина деревьев.
- `subsample` и `colsample_bytree`: Регулировка фракции данных и признаков, используемых для каждого дерева, для предотвращения переобучения.

### 5. **Оценка модели**

```python
print(f'Точность на обучающей выборке: {xgb_model.score(X_train, y_train) * 100:.2f}%')
print(f'Точность на тестовой выборке: {xgb_model.score(X_test, y_test) * 100:.2f}%')
```
![image](https://github.com/user-attachments/assets/d9294bfd-47a3-4325-ad3a-14998ff64c9a)

Оценивается точность модели XGBoost на обучающей и тестовой выборках.

### 6. **Модель стекинга**

```python
knn = KNeighborsClassifier(n_neighbors=5)
xgb = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)
meta_model = RandomForestClassifier(n_estimators=100, random_state=42)

stacking_clf = StackingClassifier(
    estimators=[('knn', knn), ('xgb', xgb)],
    final_estimator=meta_model
)
stacking_clf.fit(X_train, y_train)
```

Создается стекинговая модель, использующая два базовых классификатора:
- K-Nearest Neighbors (KNN)
- XGBoost (XGB)

Финальной моделью (мета-моделью) является классификатор RandomForest, который обучается на предсказаниях базовых моделей.

### 7. **Оценка модели стекинга**

```python
y_pred = stacking_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Точность при использовании нескольких моделей: {accuracy * 100:.2f}%')
```
На скриншоте видно, что мы достигли заметного улучшения в точности наших предсказаний.
![image](https://github.com/user-attachments/assets/959f7dc3-7acb-4b73-9195-b79339a938bd)

Оценивается точность модели стекинга на тестовой выборке. Выводится точность классификации.

## Примечания

1. **GridSearchCV** (закомментировано в коде) можно использовать для подбора гиперпараметров как для модели XGBoost, так и для модели стекинга. Это поможет найти оптимальные значения параметров, таких как `n_estimators`, `max_depth`, и `learning_rate`.

2. **Сравнение моделей**: Вы можете сравнить производительность отдельных моделей (XGBoost, KNN, Random Forest) и стекинговой модели.

3. **Масштабируемость**: Можно изменять количество деревьев и сложность моделей в зависимости от потребностей и вычислительных ограничений.

## Заключение

Этот проект демонстрирует, как применить алгоритмы машинного обучения, такие как XGBoost и стекинговые классификаторы, для предсказания болезни Паркинсона. Вы можете дополнительно оптимизировать модель, подбирая гиперпараметры или исследуя другие модели.
